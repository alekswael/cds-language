{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX4n9TsbGw-f"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0nbI5DtDGw-i"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TnJztDZGw-n"
      },
      "source": [
        "# Text classification with an RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfN3bMR5Gw-o"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/text/tutorials/text_classification_rnn\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/text_classification_rnn.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/text/blob/master/docs/tutorials/text_classification_rnn.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/text/docs/tutorials/text_classification_rnn.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUWearf0Gw-p"
      },
      "source": [
        "This text classification tutorial trains a [recurrent neural network](https://developers.google.com/machine-learning/glossary/#recurrent_neural_network) on the [IMDB large movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/) for sentiment analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2VQo4bajwUU"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "z682XYsrjkY9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "tfds.disable_progress_bar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rXHa-w9JZhb"
      },
      "source": [
        "Import `matplotlib` and create a helper function to plot graphs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Mp1Z7P9pYRSK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRmMubr0jrE2"
      },
      "source": [
        "## Setup input pipeline\n",
        "\n",
        "\n",
        "The IMDB large movie review dataset is a *binary classification* datasetâ€”all the reviews have either a *positive* or *negative* sentiment.\n",
        "\n",
        "Download the dataset using [TFDS](https://www.tensorflow.org/datasets). See the [loading text tutorial](https://www.tensorflow.org/tutorials/load_data/text) for details on how to load this sort of data manually.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SHRwRoP2nVHX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset imdb_reviews (80.23 MiB) to C:\\Users\\Aleksander\\tensorflow_datasets\\imdb_reviews\\plain_text\\0.1.0...\u001b[0m\n",
            "WARNING:tensorflow:From c:\\Users\\Aleksander\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow_datasets\\core\\file_format_adapter.py:209: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Aleksander\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow_datasets\\core\\file_format_adapter.py:209: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n",
            "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mDataset imdb_reviews downloaded and prepared to C:\\Users\\Aleksander\\tensorflow_datasets\\imdb_reviews\\plain_text\\0.1.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(), dtype=tf.int64, name=None))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset, info = tfds.load('imdb_reviews', with_info=True,\n",
        "                          as_supervised=True)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']\n",
        "\n",
        "train_dataset.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWA4c2ir7g6p"
      },
      "source": [
        "Initially this returns a dataset of (text, label pairs):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vd4_BGKyurao"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text:  b\"Oh yeah! Jenna Jameson did it again! Yeah Baby! This movie rocks. It was one of the 1st movies i saw of her. And i have to say i feel in love with her, she was great in this move.<br /><br />Her performance was outstanding and what i liked the most was the scenery and the wardrobe it was amazing you can tell that they put a lot into the movie the girls cloth were amazing.<br /><br />I hope this comment helps and u can buy the movie, the storyline is awesome is very unique and i'm sure u are going to like it. Jenna amazed us once more and no wonder the movie won so many awards. Her make-up and wardrobe is very very sexy and the girls on girls scene is amazing. specially the one where she looks like an angel. It's a must see and i hope u share my interests\"\n",
            "label:  1\n"
          ]
        }
      ],
      "source": [
        "for example, label in train_dataset.take(1):\n",
        "  print('text: ', example.numpy())\n",
        "  print('label: ', label.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2qVJzcEluH_"
      },
      "source": [
        "Next shuffle the data for training and create batches of these `(text, label)` pairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dDsCaZCDYZgm"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VznrltNOnUc5"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jqkvdcFv41wC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "texts:  [b'There\\'s a certain allure I\\'ve always found in discovering the great (semi-) unknown film. These discoveries have nearly always been dramatic films - in my experience, unknown sci-fi, action and horror are unknown for very, very good reasons. I found \"High Tide\" on video at a junk store, mixed in amongst countless dozens of tapes of varying quality. Of course, that\\'s the only place I would find it, as it is still not on DVD.<br /><br />While I was watching Judy Davis (as Lillie) throughout the course of this film, I was somehow certain I was watching a great undiscovered performance. Yes, I had previously seen Davis in several small parts - and one starring role in \"A Passage to India\". But, although she was superb in the aforementioned film, \"High Tide\" is a different animal entirely. Since recently watching Gillian Armstrong\\'s later film, \"Charlotte Gray\", I was acutely aware of the sort of actress which impresses her. Davis draws much more than a passing resemblance to Cate Blanchett - in both manner and sensibility.<br /><br />Judy Davis\\' performance is stunning, I cannot say enough good things about it. She shares an amazing on-screen relationship with young Claudia Karvan (as Ally), this film\\'s other great actress. There\\'s a lot of drama and quiet humanity they share together, the details of which I won\\'t presume to reveal here (see it for yourself!). There\\'s too much good in \"High Tide\" to cover in one review. Indeed, I would hardly care to - the film speaks well enough for itself. In conclusion, I would like to praise screenwriter Laura Jones for her stunning dialogue, director Gillian Armstrong for her understanding of actors, and the great Russell Boyd for his brilliant, understated cinematography (please see his work in \"Tender Mercies\").'\n",
            " b\"_Waterdance_ explores a wide variety of aspects of the life of the spinally injured artfully. From the petty torments of faulty fluorescent lights flashing overhead to sexuality, masculinity and depression, the experience of disability is laid open.<br /><br />The diversity of the central characters themselves underscores the complexity of the material examined - Joel, the writer, Raymond, the black man with a murky past, and Bloss, the racist biker. At first, these men are united by nothing other than the nature of their injuries, but retain their competitive spirit. Over time, shared experience, both good and bad, brings them together as friends to support one another.<br /><br />Most obvious of the transformations is that experienced by Joel, who initially distances himself from his fellow patients with sunglasses, headphones and curtains. As he comes to accept the changes that disablement has made to his life, Joel discards these props and begins to involve himself in the struggles of the men with whom he shares the ward.<br /><br />The dance referred to in the title is a reference to this daily struggle to keep one's head above water; to give up the dance is to reject life. _Waterdance_ is a moving and powerful film on many levels, and I do not hesitate to recommend it.\"\n",
            " b\"Ever watched a movie that lost the plot? Well, this didn't even really have one to begin with.<br /><br />Where to begin? The achingly tedious scenes of our heroine sitting around the house with actually no sense of menace or even foreboding created even during the apparently constant thunderstorms (that are strangely never actually heard in the house-great double glazing)? The house that is apparently only a few miles from a town yet is several hours walk away(?) or the third girl who serves no purpose to the plot except to provide a surprisingly quick gory murder just as the tedium becomes unbearable? Or even the beginning which suggests a spate of 20+ killings throughout the area even though it is apparent the killer never ventures far from the house? Or the bizarre ritual with the salt & pepper that pretty much sums up most of the films inherent lack of direction.<br /><br />Add a lead actress who can't act but at least is willing to do some completely irrelevant nude shower scenes and this video is truly nasty, but not in the way you hope.<br /><br />Given a following simply for being banned in the UK in the 80's (mostly because of a final surprisingly over extended murder) it offers nothing but curiosity value- and one classic 'daft' murder (don't worry-its telegraphed at least ten minutes before).<br /><br />After a walk in the woods our victim comes to a rather steep upward slope which they obviously struggle up. Halfway through they see a figure at the top dressed in black and brandishing a large scythe. What do they do? Slide down and run like the rest of us? No, of course not- they struggle to the top and stand conveniently nice and upright in front of the murder weapon.<br /><br />It really IS only a movie as they say..\"]\n",
            "\n",
            "labels:  [1 1 0]\n"
          ]
        }
      ],
      "source": [
        "for example, label in train_dataset.take(1):\n",
        "  print('texts: ', example.numpy()[:3])\n",
        "  print()\n",
        "  print('labels: ', label.numpy()[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5eWCo88voPY"
      },
      "source": [
        "## Create the text encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFevcItw15P_"
      },
      "source": [
        "The raw text loaded by `tfds` needs to be processed before it can be used in a model. The simplest way to process text for training is using the `TextVectorization` layer. This layer has many capabilities, but this tutorial sticks to the default behavior.\n",
        "\n",
        "Create the layer, and pass the dataset's text to the layer's `.adapt` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uC25Lu1Yvuqy"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 1000\n",
        "encoder = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE)\n",
        "encoder.adapt(train_dataset.map(lambda text, label: text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuQzVBbe3Ldu"
      },
      "source": [
        "The `.adapt` method sets the layer's vocabulary. Here are the first 20 tokens. After the padding and unknown tokens they're sorted by frequency: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tBoyjjWg0Ac9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['', '[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i',\n",
              "       'this', 'that', 'br', 'was', 'as', 'for', 'with', 'movie', 'but'],\n",
              "      dtype='<U14')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab = np.array(encoder.get_vocabulary())\n",
        "vocab[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjId5pua3jHQ"
      },
      "source": [
        "Once the vocabulary is set, the layer can encode text into indices. The tensors of indices are 0-padded to the longest sequence in the batch (unless you set a fixed `output_sequence_length`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RGc7C9WiwRWs"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[213,   4, 772, ...,   0,   0,   0],\n",
              "       [  1,   1,   4, ...,   0,   0,   0],\n",
              "       [122, 284,   4, ...,   0,   0,   0]], dtype=int64)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encoded_example = encoder(example)[:3].numpy()\n",
        "encoded_example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5cjz0bS39IN"
      },
      "source": [
        "With the default settings, the process is not completely reversible. There are three main reasons for that:\n",
        "\n",
        "1. The default value for `preprocessing.TextVectorization`'s `standardize` argument is `\"lower_and_strip_punctuation\"`.\n",
        "2. The limited vocabulary size and lack of character-based fallback results in some unknown tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "N_tD0QY5wXaK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original:  b'There\\'s a certain allure I\\'ve always found in discovering the great (semi-) unknown film. These discoveries have nearly always been dramatic films - in my experience, unknown sci-fi, action and horror are unknown for very, very good reasons. I found \"High Tide\" on video at a junk store, mixed in amongst countless dozens of tapes of varying quality. Of course, that\\'s the only place I would find it, as it is still not on DVD.<br /><br />While I was watching Judy Davis (as Lillie) throughout the course of this film, I was somehow certain I was watching a great undiscovered performance. Yes, I had previously seen Davis in several small parts - and one starring role in \"A Passage to India\". But, although she was superb in the aforementioned film, \"High Tide\" is a different animal entirely. Since recently watching Gillian Armstrong\\'s later film, \"Charlotte Gray\", I was acutely aware of the sort of actress which impresses her. Davis draws much more than a passing resemblance to Cate Blanchett - in both manner and sensibility.<br /><br />Judy Davis\\' performance is stunning, I cannot say enough good things about it. She shares an amazing on-screen relationship with young Claudia Karvan (as Ally), this film\\'s other great actress. There\\'s a lot of drama and quiet humanity they share together, the details of which I won\\'t presume to reveal here (see it for yourself!). There\\'s too much good in \"High Tide\" to cover in one review. Indeed, I would hardly care to - the film speaks well enough for itself. In conclusion, I would like to praise screenwriter Laura Jones for her stunning dialogue, director Gillian Armstrong for her understanding of actors, and the great Russell Boyd for his brilliant, understated cinematography (please see his work in \"Tender Mercies\").'\n",
            "Round-trip:  theres a certain [UNK] ive always found in [UNK] the great [UNK] [UNK] film these [UNK] have nearly always been dramatic films in my experience [UNK] scifi action and horror are [UNK] for very very good [UNK] i found high [UNK] on video at a [UNK] [UNK] [UNK] in [UNK] [UNK] [UNK] of [UNK] of [UNK] quality of course thats the only place i would find it as it is still not on [UNK] br while i was watching [UNK] [UNK] as [UNK] throughout the course of this film i was somehow certain i was watching a great [UNK] performance yes i had [UNK] seen [UNK] in several small parts and one [UNK] role in a [UNK] to [UNK] but although she was superb in the [UNK] film high [UNK] is a different [UNK] [UNK] since [UNK] watching [UNK] [UNK] later film [UNK] [UNK] i was [UNK] [UNK] of the sort of actress which [UNK] her [UNK] [UNK] much more than a [UNK] [UNK] to [UNK] [UNK] in both [UNK] and [UNK] br [UNK] [UNK] performance is [UNK] i cannot say enough good things about it she [UNK] an amazing [UNK] relationship with young [UNK] [UNK] as [UNK] this films other great actress theres a lot of drama and [UNK] [UNK] they [UNK] together the [UNK] of which i wont [UNK] to [UNK] here see it for yourself theres too much good in high [UNK] to [UNK] in one review indeed i would hardly care to the film [UNK] well enough for itself in [UNK] i would like to [UNK] [UNK] [UNK] [UNK] for her [UNK] dialogue director [UNK] [UNK] for her [UNK] of actors and the great [UNK] [UNK] for his brilliant [UNK] cinematography please see his work in [UNK] [UNK]                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
            "\n",
            "Original:  b\"_Waterdance_ explores a wide variety of aspects of the life of the spinally injured artfully. From the petty torments of faulty fluorescent lights flashing overhead to sexuality, masculinity and depression, the experience of disability is laid open.<br /><br />The diversity of the central characters themselves underscores the complexity of the material examined - Joel, the writer, Raymond, the black man with a murky past, and Bloss, the racist biker. At first, these men are united by nothing other than the nature of their injuries, but retain their competitive spirit. Over time, shared experience, both good and bad, brings them together as friends to support one another.<br /><br />Most obvious of the transformations is that experienced by Joel, who initially distances himself from his fellow patients with sunglasses, headphones and curtains. As he comes to accept the changes that disablement has made to his life, Joel discards these props and begins to involve himself in the struggles of the men with whom he shares the ward.<br /><br />The dance referred to in the title is a reference to this daily struggle to keep one's head above water; to give up the dance is to reject life. _Waterdance_ is a moving and powerful film on many levels, and I do not hesitate to recommend it.\"\n",
            "Round-trip:  [UNK] [UNK] a [UNK] [UNK] of [UNK] of the life of the [UNK] [UNK] [UNK] from the [UNK] [UNK] of [UNK] [UNK] [UNK] [UNK] [UNK] to [UNK] [UNK] and [UNK] the experience of [UNK] is [UNK] [UNK] br the [UNK] of the [UNK] characters themselves [UNK] the [UNK] of the material [UNK] [UNK] the writer [UNK] the black man with a [UNK] past and [UNK] the [UNK] [UNK] at first these men are [UNK] by nothing other than the nature of their [UNK] but [UNK] their [UNK] [UNK] over time [UNK] experience both good and bad brings them together as friends to [UNK] one [UNK] br most obvious of the [UNK] is that [UNK] by [UNK] who [UNK] [UNK] himself from his [UNK] [UNK] with [UNK] [UNK] and [UNK] as he comes to [UNK] the [UNK] that [UNK] has made to his life [UNK] [UNK] these [UNK] and begins to [UNK] himself in the [UNK] of the men with whom he [UNK] the [UNK] br the dance [UNK] to in the title is a [UNK] to this [UNK] [UNK] to keep ones head above [UNK] to give up the dance is to [UNK] life [UNK] is a moving and powerful film on many [UNK] and i do not [UNK] to recommend it                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
            "\n",
            "Original:  b\"Ever watched a movie that lost the plot? Well, this didn't even really have one to begin with.<br /><br />Where to begin? The achingly tedious scenes of our heroine sitting around the house with actually no sense of menace or even foreboding created even during the apparently constant thunderstorms (that are strangely never actually heard in the house-great double glazing)? The house that is apparently only a few miles from a town yet is several hours walk away(?) or the third girl who serves no purpose to the plot except to provide a surprisingly quick gory murder just as the tedium becomes unbearable? Or even the beginning which suggests a spate of 20+ killings throughout the area even though it is apparent the killer never ventures far from the house? Or the bizarre ritual with the salt & pepper that pretty much sums up most of the films inherent lack of direction.<br /><br />Add a lead actress who can't act but at least is willing to do some completely irrelevant nude shower scenes and this video is truly nasty, but not in the way you hope.<br /><br />Given a following simply for being banned in the UK in the 80's (mostly because of a final surprisingly over extended murder) it offers nothing but curiosity value- and one classic 'daft' murder (don't worry-its telegraphed at least ten minutes before).<br /><br />After a walk in the woods our victim comes to a rather steep upward slope which they obviously struggle up. Halfway through they see a figure at the top dressed in black and brandishing a large scythe. What do they do? Slide down and run like the rest of us? No, of course not- they struggle to the top and stand conveniently nice and upright in front of the murder weapon.<br /><br />It really IS only a movie as they say..\"\n",
            "Round-trip:  ever watched a movie that lost the plot well this didnt even really have one to begin [UNK] br where to begin the [UNK] [UNK] scenes of our [UNK] [UNK] around the house with actually no sense of [UNK] or even [UNK] [UNK] even during the apparently [UNK] [UNK] that are [UNK] never actually heard in the [UNK] [UNK] [UNK] the house that is apparently only a few [UNK] from a town yet is several hours [UNK] away or the third girl who [UNK] no [UNK] to the plot except to [UNK] a [UNK] [UNK] [UNK] murder just as the [UNK] becomes [UNK] or even the beginning which [UNK] a [UNK] of 20 [UNK] throughout the [UNK] even though it is [UNK] the killer never [UNK] far from the house or the [UNK] [UNK] with the [UNK] [UNK] that pretty much [UNK] up most of the films [UNK] lack of [UNK] br add a lead actress who cant act but at least is [UNK] to do some completely [UNK] [UNK] [UNK] scenes and this video is truly [UNK] but not in the way you [UNK] br given a [UNK] simply for being [UNK] in the [UNK] in the 80s mostly because of a final [UNK] over [UNK] murder it [UNK] nothing but [UNK] [UNK] and one classic [UNK] murder dont [UNK] [UNK] at least ten minutes [UNK] br after a [UNK] in the [UNK] our [UNK] comes to a rather [UNK] [UNK] [UNK] which they obviously [UNK] up [UNK] through they see a figure at the top [UNK] in black and [UNK] a [UNK] [UNK] what do they do [UNK] down and run like the rest of us no of course not they [UNK] to the top and stand [UNK] nice and [UNK] in [UNK] of the murder [UNK] br it really is only a movie as they say                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for n in range(3):\n",
        "  print(\"Original: \", example[n].numpy())\n",
        "  print(\"Round-trip: \", \" \".join(vocab[encoded_example[n]]))\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjUqGVBxGw-t"
      },
      "source": [
        "## Create the model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "W7zsmInBOCPO"
      },
      "source": [
        "![A drawing of the information flow in the model](bidirectional.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgs6nnSTGw-t"
      },
      "source": [
        "Above is a diagram of the model. \n",
        "\n",
        "1. This model can be build as a `tf.keras.Sequential`.\n",
        "\n",
        "2. The first layer is the `encoder`, which converts the text to a sequence of token indices.\n",
        "\n",
        "3. After the encoder is an embedding layer. An embedding layer stores one vector per word. When called, it converts the sequences of word indices to sequences of vectors. These vectors are trainable. After training (on enough data), words with similar meanings often have similar vectors.\n",
        "\n",
        "  This index-lookup is much more efficient than the equivalent operation of passing a one-hot encoded vector through a `tf.keras.layers.Dense` layer.\n",
        "\n",
        "4. A recurrent neural network (RNN) processes sequence input by iterating through the elements. RNNs pass the outputs from one timestep to their input on the next timestep.\n",
        "\n",
        "  The `tf.keras.layers.Bidirectional` wrapper can also be used with an RNN layer. This propagates the input forward and backwards through the RNN layer and then concatenates the final output. \n",
        "\n",
        "  * The main advantage of a bidirectional RNN is that the signal from the beginning of the input doesn't need to be processed all the way through every timestep to affect the output.  \n",
        "\n",
        "  * The main disadvantage of a bidirectional RNN is that you can't efficiently stream predictions as words are being added to the end.\n",
        "\n",
        "5. After the RNN has converted the sequence to a single vector the two `layers.Dense` do some final processing, and convert from this vector representation to a single logit as the classification output. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4fodCI7soQi"
      },
      "source": [
        "The code to implement this is below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LwfoBkmRYcP3"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=len(encoder.get_vocabulary()),\n",
        "        output_dim=64,\n",
        "        # Use masking to handle the variable sequence lengths\n",
        "        mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIGmIGkkouUb"
      },
      "source": [
        "Please note that Keras sequential model is used here since all the layers in the model only have single input and produce single output. In case you want to use stateful RNN layer, you might want to build your model with Keras functional API or model subclassing so that you can retrieve and reuse the RNN layer states. Please check [Keras RNN guide](https://www.tensorflow.org/guide/keras/rnn#rnn_state_reuse) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF-PsCk1LwjY"
      },
      "source": [
        "The embedding layer [uses masking](https://www.tensorflow.org/guide/keras/masking_and_padding) to handle the varying sequence-lengths. All the layers after the `Embedding` support masking:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "87a8-CwfKebw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[False, True, True, True, True]\n"
          ]
        }
      ],
      "source": [
        "print([layer.supports_masking for layer in model.layers])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlS0iaUIWLpI"
      },
      "source": [
        "To confirm that this works as expected, evaluate a sentence twice. First, alone so there's no padding to mask:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "O41gw3KfWHus"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.00148002]\n"
          ]
        }
      ],
      "source": [
        "# predict on a sample text without padding.\n",
        "\n",
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0VQmGnEWcuz"
      },
      "source": [
        "Now, evaluate it again in a batch with a longer sentence. The result should be identical:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UIgpuTeFNDzq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.00148003]\n"
          ]
        }
      ],
      "source": [
        "# predict on a sample text with padding\n",
        "\n",
        "padding = \"the \" * 2000\n",
        "predictions = model.predict(np.array([sample_text, padding]))\n",
        "print(predictions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRI776ZcH3Tf"
      },
      "source": [
        "Compile the Keras model to configure the training process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "kj2xei41YZjC"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIwH3nto596k"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hw86wWS4YgR2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "391/391 [==============================] - 822s 2s/step - loss: 0.6527 - accuracy: 0.5562 - val_loss: 0.5491 - val_accuracy: 0.7531\n",
            "Epoch 2/2\n",
            "391/391 [==============================] - 788s 2s/step - loss: 0.4446 - accuracy: 0.7867 - val_loss: 0.3981 - val_accuracy: 0.7906\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(train_dataset, epochs=2,\n",
        "                    validation_data=test_dataset,\n",
        "                    validation_steps=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BaNbXi43YgUT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 208s 532ms/step - loss: 0.4049 - accuracy: 0.7965\n",
            "Test Loss: 0.40490487217903137\n",
            "Test Accuracy: 0.7964800000190735\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OZmwt_mzaQJk"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'history' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m16\u001b[39m, \u001b[39m8\u001b[39m))\n\u001b[0;32m      2\u001b[0m plt\u001b[39m.\u001b[39msubplot(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m plot_graphs(history, \u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[39m.\u001b[39mylim(\u001b[39mNone\u001b[39;00m, \u001b[39m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m plt\u001b[39m.\u001b[39msubplot(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1600x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.ylim(None, 1)\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'loss')\n",
        "plt.ylim(0, None)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![output](output.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwSE_386uhxD"
      },
      "source": [
        "Run a prediction on a new sentence:\n",
        "\n",
        "If the prediction is >= 0.0, it is positive else it is negative."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXgfQSgRW6zU"
      },
      "outputs": [],
      "source": [
        "sample_text = ('The movie was cool. The animation and the graphics '\n",
        "               'were out of this world. I would recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7g1evcaRpTKm"
      },
      "source": [
        "## Stack two or more LSTM layers\n",
        "\n",
        "Keras recurrent layers have two available modes that are controlled by the `return_sequences` constructor argument:\n",
        "\n",
        "* If `False` it returns only the last output for each input sequence (a 2D tensor of shape (batch_size, output_features)). This is the default, used in the previous model.\n",
        "\n",
        "* If `True` the full sequences of successive outputs for each timestep is returned (a 3D tensor of shape `(batch_size, timesteps, output_features)`).\n",
        "\n",
        "Here is what the flow of information looks like with `return_sequences=True`:\n",
        "\n",
        "![layered_bidirectional](layered_bidirectional.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbSClCrG1z8l"
      },
      "source": [
        "The interesting thing about using an `RNN` with `return_sequences=True` is that the output still has 3-axes, like the input, so it can be passed to another RNN layer, like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jo1jjO3vn0jo"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEPV5jVGp-is"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeSE-YjdqAeN"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_dataset, epochs=10,\n",
        "                    validation_data=test_dataset,\n",
        "                    validation_steps=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LdwilM1qPM3"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykUKnAoqbycW"
      },
      "outputs": [],
      "source": [
        "# predict on a sample text without padding.\n",
        "\n",
        "sample_text = ('The movie was not good. The animation and the graphics '\n",
        "               'were terrible. I would not recommend this movie.')\n",
        "predictions = model.predict(np.array([sample_text]))\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YYub0EDtwCu"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(history, 'accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(history, 'loss')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![output_2](output_2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xvpE3BaGw_V"
      },
      "source": [
        "Check out other existing recurrent layers such as [GRU layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU).\n",
        "\n",
        "If you're interested in building custom RNNs, see the [Keras RNN Guide](https://www.tensorflow.org/guide/keras/rnn).\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "text_classification_rnn.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
