{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 11 - Generative language models for zero-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to be working with ```FLAN-T5```, a text-to-text model developed by Google. ```FLAN-T5``` is based on ```T5```, which we saw in the lecture, but it has been further finetuned on a range of common text-to-text tasks. This means that it can already perform a lot of the kinds of tasks that people use generative language models for. You can read more in the paper here: [https://arxiv.org/pdf/2210.11416.pdf](https://arxiv.org/pdf/2210.11416.pdf)\n",
    "\n",
    "We load the model from ```huggingface```. We're here using the ```Large``` version, but you can try the other sizes if you want. The ```Large``` version is already 3.4GB, and even larger models will take a long time to download and to run - but they should also see a marked performance improvement.\n",
    " \n",
    "You can read more about the available models here: [https://huggingface.co/docs/transformers/model_doc/flan-t5](https://huggingface.co/docs/transformers/model_doc/flan-t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at google/flan-t5-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also download and initalize the pretrained tokenizer that fits with our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt engineering\n",
    "\n",
    "Our goal is to use the knowledge of language that FLAN-T5 has already acquired during training and to use that knowledge in different domains *without any further fine-tuning*. This is an example of what is called *zero-shot* learning.\n",
    "\n",
    "In order for zero-shot learning to be successful, our prompts need to be carefully designed. FLAN-T5 (and similar models) are a bit less flexible than, for example, ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification\n",
    "prompt = \"classify the following text as positive or negative: I absolutely hated this movie\"\n",
    "\n",
    "# translation\n",
    "#prompt = \"translate from English to French: how old are you?\"\n",
    "\n",
    "# question answering\n",
    "#prompt = \"answer the following question: how is cheese made?\"\n",
    "\n",
    "# named entity recognition\n",
    "#prompt = \"find all location entities in this text: Ross comes from Scotland\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pass our text prompt to the tokenizer, defing some extra arguments such as the ```max_length``` of our input (anything longer than this will be truncated):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt,\n",
    "                   max_length = 200,\n",
    "                   return_tensors=\"tf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pass all of our input prompt tokens to the model and use than to generate an appropriate output from what FLAN-T5 has learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aleksander\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\generation_tf_utils.py:1694: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Exception encountered when calling layer \"SelfAttention\" (type TFT5Attention).\n\nCould not find compiler for platform Host: NOT_FOUND: could not find registered compiler for platform Host -- check target linkage (hint: try adding tensorflow/compiler/jit:xla_cpu_jit as a dependency) [Op:XlaDynamicSlice]\n\nCall arguments received:\n  • hidden_states=tf.Tensor(shape=(1, 1, 1024), dtype=float32)\n  • mask=tf.Tensor(shape=(1, 1, 1, 2), dtype=float32)\n  • key_value_states=None\n  • position_bias=None\n  • past_key_value=('tf.Tensor(shape=(1, 16, 1, 64), dtype=float32)', 'tf.Tensor(shape=(1, 16, 1, 64), dtype=float32)')\n  • layer_head_mask=None\n  • query_length=None\n  • use_cache=True\n  • training=False\n  • output_attentions=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n",
      "File \u001b[1;32mc:\\Users\\Aleksander\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\generation_tf_utils.py:600\u001b[0m, in \u001b[0;36mTFGenerationMixin.generate\u001b[1;34m(self, input_ids, max_length, max_new_tokens, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, output_scores, output_attentions, output_hidden_states, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[39mif\u001b[39;00m do_sample \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m num_beams \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    599\u001b[0m     seed \u001b[39m=\u001b[39m model_kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mseed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 600\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(\n\u001b[0;32m    601\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m    602\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m    603\u001b[0m         max_new_tokens\u001b[39m=\u001b[39mmax_new_tokens,\n\u001b[0;32m    604\u001b[0m         min_length\u001b[39m=\u001b[39mmin_length,\n\u001b[0;32m    605\u001b[0m         do_sample\u001b[39m=\u001b[39mdo_sample,\n\u001b[0;32m    606\u001b[0m         early_stopping\u001b[39m=\u001b[39mearly_stopping,\n\u001b[0;32m    607\u001b[0m         num_beams\u001b[39m=\u001b[39mnum_beams,\n\u001b[0;32m    608\u001b[0m         temperature\u001b[39m=\u001b[39mtemperature,\n\u001b[0;32m    609\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[0;32m    610\u001b[0m         top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[0;32m    611\u001b[0m         repetition_penalty\u001b[39m=\u001b[39mrepetition_penalty,\n\u001b[0;32m    612\u001b[0m         bad_words_ids\u001b[39m=\u001b[39mbad_words_ids,\n\u001b[0;32m    613\u001b[0m         bos_token_id\u001b[39m=\u001b[39mbos_token_id,\n\u001b[0;32m    614\u001b[0m         pad_token_id\u001b[39m=\u001b[39mpad_token_id,\n\u001b[0;32m    615\u001b[0m         eos_token_id\u001b[39m=\u001b[39meos_token_id,\n\u001b[0;32m    616\u001b[0m         length_penalty\u001b[39m=\u001b[39mlength_penalty,\n\u001b[0;32m    617\u001b[0m         no_repeat_ngram_size\u001b[39m=\u001b[39mno_repeat_ngram_size,\n\u001b[0;32m    618\u001b[0m         num_return_sequences\u001b[39m=\u001b[39mnum_return_sequences,\n\u001b[0;32m    619\u001b[0m         attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m    620\u001b[0m         decoder_start_token_id\u001b[39m=\u001b[39mdecoder_start_token_id,\n\u001b[0;32m    621\u001b[0m         use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[0;32m    622\u001b[0m         seed\u001b[39m=\u001b[39mseed,\n\u001b[0;32m    623\u001b[0m         output_scores\u001b[39m=\u001b[39moutput_scores,\n\u001b[0;32m    624\u001b[0m         output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m    625\u001b[0m         output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    626\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m    627\u001b[0m         forced_bos_token_id\u001b[39m=\u001b[39mforced_bos_token_id,\n\u001b[0;32m    628\u001b[0m         forced_eos_token_id\u001b[39m=\u001b[39mforced_eos_token_id,\n\u001b[0;32m    629\u001b[0m         suppress_tokens\u001b[39m=\u001b[39msuppress_tokens,\n\u001b[0;32m    630\u001b[0m         begin_suppress_tokens\u001b[39m=\u001b[39mbegin_suppress_tokens,\n\u001b[0;32m    631\u001b[0m         forced_decoder_ids\u001b[39m=\u001b[39mforced_decoder_ids,\n\u001b[0;32m    632\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    633\u001b[0m     )\n\u001b[0;32m    635\u001b[0m \u001b[39m# We cannot generate if the model does not have a LM head\u001b[39;00m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_output_embeddings() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Aleksander\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\generation_tf_utils.py:1756\u001b[0m, in \u001b[0;36mTFGenerationMixin._generate\u001b[1;34m(self, input_ids, max_length, max_new_tokens, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, seed, output_scores, output_attentions, output_hidden_states, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[0;32m   1752\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1753\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnum_return_sequences has to be 1, but is \u001b[39m\u001b[39m{\u001b[39;00mnum_return_sequences\u001b[39m}\u001b[39;00m\u001b[39m when doing greedy search.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1754\u001b[0m         )\n\u001b[0;32m   1755\u001b[0m     \u001b[39m# 9. run greedy search\u001b[39;00m\n\u001b[1;32m-> 1756\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgreedy_search(\n\u001b[0;32m   1757\u001b[0m         input_ids,\n\u001b[0;32m   1758\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   1759\u001b[0m         pad_token_id\u001b[39m=\u001b[39mpad_token_id,\n\u001b[0;32m   1760\u001b[0m         eos_token_id\u001b[39m=\u001b[39meos_token_id,\n\u001b[0;32m   1761\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[0;32m   1762\u001b[0m         output_scores\u001b[39m=\u001b[39moutput_scores,\n\u001b[0;32m   1763\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39mreturn_dict_in_generate,\n\u001b[0;32m   1764\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   1765\u001b[0m     )\n\u001b[0;32m   1766\u001b[0m \u001b[39melif\u001b[39;00m is_sample_gen_mode:\n\u001b[0;32m   1767\u001b[0m     \u001b[39m# 10. prepare logits warper\u001b[39;00m\n\u001b[0;32m   1768\u001b[0m     logits_warper \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_logits_warper(top_k\u001b[39m=\u001b[39mtop_k, top_p\u001b[39m=\u001b[39mtop_p, temperature\u001b[39m=\u001b[39mtemperature)\n",
      "File \u001b[1;32mc:\\Users\\Aleksander\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\generation_tf_utils.py:2405\u001b[0m, in \u001b[0;36mTFGenerationMixin.greedy_search\u001b[1;34m(self, input_ids, max_length, pad_token_id, eos_token_id, logits_processor, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[0m\n\u001b[0;32m   2403\u001b[0m \u001b[39mif\u001b[39;00m greedy_search_cond_fn(generated, finished_sequences, cur_len, model_kwargs):\n\u001b[0;32m   2404\u001b[0m     maximum_iterations \u001b[39m=\u001b[39m max_length \u001b[39m-\u001b[39m cur_len\n\u001b[1;32m-> 2405\u001b[0m     generated, _, cur_len, _ \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mwhile_loop(\n\u001b[0;32m   2406\u001b[0m         greedy_search_cond_fn,\n\u001b[0;32m   2407\u001b[0m         greedy_search_body_fn,\n\u001b[0;32m   2408\u001b[0m         (generated, finished_sequences, cur_len, model_kwargs),\n\u001b[0;32m   2409\u001b[0m         maximum_iterations\u001b[39m=\u001b[39;49mmaximum_iterations,\n\u001b[0;32m   2410\u001b[0m     )\n\u001b[0;32m   2412\u001b[0m \u001b[39m# 6. prepare outputs\u001b[39;00m\n\u001b[0;32m   2413\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m use_xla:\n\u001b[0;32m   2414\u001b[0m     \u001b[39m# cut for backward compatibility\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aleksander\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:616\u001b[0m, in \u001b[0;36mdeprecated_arg_values.<locals>.deprecated_wrapper.<locals>.new_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    609\u001b[0m           _PRINTED_WARNING[(func, arg_name)] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    610\u001b[0m         logging\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    611\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mFrom \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: calling \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m (from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) with \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is deprecated and \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    612\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mwill be removed \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mInstructions for updating:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[0;32m    613\u001b[0m             _call_location(), decorator_utils\u001b[39m.\u001b[39mget_qualified_name(func),\n\u001b[0;32m    614\u001b[0m             func\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m, arg_name, arg_value, \u001b[39m'\u001b[39m\u001b[39min a future version\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    615\u001b[0m             \u001b[39mif\u001b[39;00m date \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mafter \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m date), instructions)\n\u001b[1;32m--> 616\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Aleksander\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:2548\u001b[0m, in \u001b[0;36mwhile_loop_v2\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[0;32m   2372\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mwhile_loop\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[0;32m   2373\u001b[0m \u001b[39m@deprecation\u001b[39m\u001b[39m.\u001b[39mdeprecated_arg_values(\n\u001b[0;32m   2374\u001b[0m     \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2389\u001b[0m                   maximum_iterations\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   2390\u001b[0m                   name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   2391\u001b[0m   \u001b[39m\"\"\"Repeat `body` while the condition `cond` is true.\u001b[39;00m\n\u001b[0;32m   2392\u001b[0m \n\u001b[0;32m   2393\u001b[0m \u001b[39m  `cond` is a callable returning a boolean scalar tensor. `body` is a callable\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2546\u001b[0m \n\u001b[0;32m   2547\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2548\u001b[0m   \u001b[39mreturn\u001b[39;00m while_loop(\n\u001b[0;32m   2549\u001b[0m       cond\u001b[39m=\u001b[39;49mcond,\n\u001b[0;32m   2550\u001b[0m       body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m   2551\u001b[0m       loop_vars\u001b[39m=\u001b[39;49mloop_vars,\n\u001b[0;32m   2552\u001b[0m       shape_invariants\u001b[39m=\u001b[39;49mshape_invariants,\n\u001b[0;32m   2553\u001b[0m       parallel_iterations\u001b[39m=\u001b[39;49mparallel_iterations,\n\u001b[0;32m   2554\u001b[0m       back_prop\u001b[39m=\u001b[39;49mback_prop,\n\u001b[0;32m   2555\u001b[0m       swap_memory\u001b[39m=\u001b[39;49mswap_memory,\n\u001b[0;32m   2556\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m   2557\u001b[0m       maximum_iterations\u001b[39m=\u001b[39;49mmaximum_iterations,\n\u001b[0;32m   2558\u001b[0m       return_same_structure\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\Aleksander\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:2795\u001b[0m, in \u001b[0;36mwhile_loop\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[0;32m   2792\u001b[0m loop_var_structure \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mmap_structure(type_spec\u001b[39m.\u001b[39mtype_spec_from_value,\n\u001b[0;32m   2793\u001b[0m                                         \u001b[39mlist\u001b[39m(loop_vars))\n\u001b[0;32m   2794\u001b[0m \u001b[39mwhile\u001b[39;00m cond(\u001b[39m*\u001b[39mloop_vars):\n\u001b[1;32m-> 2795\u001b[0m   loop_vars \u001b[39m=\u001b[39m body(\u001b[39m*\u001b[39;49mloop_vars)\n\u001b[0;32m   2796\u001b[0m   \u001b[39mif\u001b[39;00m try_to_pack \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(loop_vars, (\u001b[39mlist\u001b[39m, _basetuple)):\n\u001b[0;32m   2797\u001b[0m     packed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aleksander\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:2786\u001b[0m, in \u001b[0;36mwhile_loop.<locals>.<lambda>\u001b[1;34m(i, lv)\u001b[0m\n\u001b[0;32m   2783\u001b[0m     loop_vars \u001b[39m=\u001b[39m (counter, loop_vars)\n\u001b[0;32m   2784\u001b[0m     cond \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m i, lv: (  \u001b[39m# pylint: disable=g-long-lambda\u001b[39;00m\n\u001b[0;32m   2785\u001b[0m         math_ops\u001b[39m.\u001b[39mlogical_and(i \u001b[39m<\u001b[39m maximum_iterations, orig_cond(\u001b[39m*\u001b[39mlv)))\n\u001b[1;32m-> 2786\u001b[0m     body \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m i, lv: (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, orig_body(\u001b[39m*\u001b[39;49mlv))\n\u001b[0;32m   2787\u001b[0m   try_to_pack \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   2789\u001b[0m \u001b[39mif\u001b[39;00m executing_eagerly:\n",
      "File \u001b[1;32mc:\\Users\\Aleksander\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\generation_tf_utils.py:2331\u001b[0m, in \u001b[0;36mTFGenerationMixin.greedy_search.<locals>.greedy_search_body_fn\u001b[1;34m(generated, finished_sequences, cur_len, model_kwargs)\u001b[0m\n\u001b[0;32m   2329\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   2330\u001b[0m \u001b[39m# forward pass to get next token logits\u001b[39;00m\n\u001b[1;32m-> 2331\u001b[0m model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(\n\u001b[0;32m   2332\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_inputs,\n\u001b[0;32m   2333\u001b[0m     return_dict\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   2334\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m   2335\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   2336\u001b[0m )\n\u001b[0;32m   2337\u001b[0m next_token_logits \u001b[39m=\u001b[39m model_outputs\u001b[39m.\u001b[39mlogits[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m   2339\u001b[0m \u001b[39m# Store scores, attentions and hidden_states when required\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aleksander\\anaconda3\\envs\\NLP\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Aleksander\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\modeling_tf_utils.py:420\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\n\u001b[0;32m    419\u001b[0m unpacked_inputs \u001b[39m=\u001b[39m input_processing(func, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_args_and_kwargs)\n\u001b[1;32m--> 420\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munpacked_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Aleksander\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\models\\t5\\modeling_tf_t5.py:1414\u001b[0m, in \u001b[0;36mTFT5ForConditionalGeneration.call\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m   1411\u001b[0m     decoder_input_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shift_right(labels)\n\u001b[0;32m   1413\u001b[0m \u001b[39m# Decode\u001b[39;00m\n\u001b[1;32m-> 1414\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[0;32m   1415\u001b[0m     decoder_input_ids,\n\u001b[0;32m   1416\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[0;32m   1417\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[0;32m   1418\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1419\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[0;32m   1420\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[0;32m   1421\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1422\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1423\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1424\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1425\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1426\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[0;32m   1427\u001b[0m )\n\u001b[0;32m   1429\u001b[0m sequence_output \u001b[39m=\u001b[39m decoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1431\u001b[0m \u001b[39m# T5v1.1 does not tie output word embeddings and thus does not require downscaling\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aleksander\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\modeling_tf_utils.py:420\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\n\u001b[0;32m    419\u001b[0m unpacked_inputs \u001b[39m=\u001b[39m input_processing(func, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_args_and_kwargs)\n\u001b[1;32m--> 420\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munpacked_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Aleksander\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\models\\t5\\modeling_tf_t5.py:789\u001b[0m, in \u001b[0;36mTFT5MainLayer.call\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, encoder_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    788\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_states,)\n\u001b[1;32m--> 789\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    790\u001b[0m     hidden_states,\n\u001b[0;32m    791\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m    792\u001b[0m     position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[0;32m    793\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    794\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m    795\u001b[0m     encoder_decoder_position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[0;32m    796\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mhead_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    797\u001b[0m     encoder_layer_head_mask\u001b[39m=\u001b[39;49mencoder_head_mask[idx] \u001b[39mif\u001b[39;49;00m encoder_head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    798\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m    799\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    800\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    801\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[0;32m    802\u001b[0m )\n\u001b[0;32m    804\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)\u001b[39;00m\n\u001b[0;32m    806\u001b[0m hidden_states, present_key_value_state \u001b[39m=\u001b[39m layer_outputs[:\u001b[39m2\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Aleksander\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\models\\t5\\modeling_tf_t5.py:568\u001b[0m, in \u001b[0;36mTFT5Block.call\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, encoder_layer_head_mask, past_key_value, use_cache, output_attentions, training)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    566\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 568\u001b[0m self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m0\u001b[39;49m](\n\u001b[0;32m    569\u001b[0m     hidden_states,\n\u001b[0;32m    570\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    571\u001b[0m     position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[0;32m    572\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[0;32m    573\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[0;32m    574\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    575\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    576\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[0;32m    577\u001b[0m )\n\u001b[0;32m    578\u001b[0m hidden_states, present_key_value_state \u001b[39m=\u001b[39m self_attention_outputs[:\u001b[39m2\u001b[39m]\n\u001b[0;32m    579\u001b[0m attention_outputs \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m2\u001b[39m:]  \u001b[39m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aleksander\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\models\\t5\\modeling_tf_t5.py:457\u001b[0m, in \u001b[0;36mTFT5LayerSelfAttention.call\u001b[1;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions, training)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    447\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    454\u001b[0m     training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    455\u001b[0m ):\n\u001b[0;32m    456\u001b[0m     normed_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m--> 457\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mSelfAttention(\n\u001b[0;32m    458\u001b[0m         normed_hidden_states,\n\u001b[0;32m    459\u001b[0m         mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    460\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[0;32m    461\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[0;32m    462\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m    463\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    464\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    465\u001b[0m         training\u001b[39m=\u001b[39;49mtraining,\n\u001b[0;32m    466\u001b[0m     )\n\u001b[0;32m    467\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention_output[\u001b[39m0\u001b[39m], training\u001b[39m=\u001b[39mtraining)\n\u001b[0;32m    468\u001b[0m     outputs \u001b[39m=\u001b[39m (hidden_states,) \u001b[39m+\u001b[39m attention_output[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aleksander\\anaconda3\\envs\\NLP\\lib\\site-packages\\transformers\\models\\t5\\modeling_tf_t5.py:396\u001b[0m, in \u001b[0;36mTFT5Attention.call\u001b[1;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, training, output_attentions)\u001b[0m\n\u001b[0;32m    392\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m         \u001b[39m# we might have a padded past structure, in which case we want to fetch the position bias slice\u001b[39;00m\n\u001b[0;32m    394\u001b[0m         \u001b[39m# right after the most recently filled past index\u001b[39;00m\n\u001b[0;32m    395\u001b[0m         most_recently_filled_past_index \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_max(tf\u001b[39m.\u001b[39mwhere(past_key_value[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, :, \u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m0.0\u001b[39m))\n\u001b[1;32m--> 396\u001b[0m         position_bias \u001b[39m=\u001b[39m dynamic_slice(\n\u001b[0;32m    397\u001b[0m             position_bias,\n\u001b[0;32m    398\u001b[0m             (\u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m, most_recently_filled_past_index \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, \u001b[39m0\u001b[39;49m),\n\u001b[0;32m    399\u001b[0m             (\u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_heads, seq_length, real_seq_length),\n\u001b[0;32m    400\u001b[0m         )\n\u001b[0;32m    402\u001b[0m \u001b[39mif\u001b[39;00m mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m     position_bias \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(position_bias, dtype\u001b[39m=\u001b[39mmask\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\Aleksander\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\compiler\\tf2xla\\ops\\gen_xla_ops.py:928\u001b[0m, in \u001b[0;36mxla_dynamic_slice\u001b[1;34m(input, start_indices, size_indices, name)\u001b[0m\n\u001b[0;32m    926\u001b[0m   \u001b[39mif\u001b[39;00m _result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[0;32m    927\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m--> 928\u001b[0m   \u001b[39mreturn\u001b[39;00m xla_dynamic_slice_eager_fallback(\n\u001b[0;32m    929\u001b[0m       \u001b[39minput\u001b[39m, start_indices, size_indices, name\u001b[39m=\u001b[39mname, ctx\u001b[39m=\u001b[39m_ctx)\n\u001b[0;32m    930\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_SymbolicException:\n\u001b[0;32m    931\u001b[0m   \u001b[39mpass\u001b[39;00m  \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Aleksander\\anaconda3\\envs\\NLP\\lib\\site-packages\\tensorflow\\compiler\\tf2xla\\ops\\gen_xla_ops.py:980\u001b[0m, in \u001b[0;36mxla_dynamic_slice_eager_fallback\u001b[1;34m(input, start_indices, size_indices, name, ctx)\u001b[0m\n\u001b[0;32m    978\u001b[0m _inputs_flat \u001b[39m=\u001b[39m [\u001b[39minput\u001b[39m, start_indices, size_indices]\n\u001b[0;32m    979\u001b[0m _attrs \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mT\u001b[39m\u001b[39m\"\u001b[39m, _attr_T, \u001b[39m\"\u001b[39m\u001b[39mTindices\u001b[39m\u001b[39m\"\u001b[39m, _attr_Tindices)\n\u001b[1;32m--> 980\u001b[0m _result \u001b[39m=\u001b[39m _execute\u001b[39m.\u001b[39;49mexecute(\u001b[39mb\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mXlaDynamicSlice\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m1\u001b[39;49m, inputs\u001b[39m=\u001b[39;49m_inputs_flat,\n\u001b[0;32m    981\u001b[0m                            attrs\u001b[39m=\u001b[39;49m_attrs, ctx\u001b[39m=\u001b[39;49mctx, name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m    982\u001b[0m \u001b[39mif\u001b[39;00m _execute\u001b[39m.\u001b[39mmust_record_gradient():\n\u001b[0;32m    983\u001b[0m   _execute\u001b[39m.\u001b[39mrecord_gradient(\n\u001b[0;32m    984\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mXlaDynamicSlice\u001b[39m\u001b[39m\"\u001b[39m, _inputs_flat, _attrs, _result)\n",
      "\u001b[1;31mUnimplementedError\u001b[0m: Exception encountered when calling layer \"SelfAttention\" (type TFT5Attention).\n\nCould not find compiler for platform Host: NOT_FOUND: could not find registered compiler for platform Host -- check target linkage (hint: try adding tensorflow/compiler/jit:xla_cpu_jit as a dependency) [Op:XlaDynamicSlice]\n\nCall arguments received:\n  • hidden_states=tf.Tensor(shape=(1, 1, 1024), dtype=float32)\n  • mask=tf.Tensor(shape=(1, 1, 1, 2), dtype=float32)\n  • key_value_states=None\n  • position_bias=None\n  • past_key_value=('tf.Tensor(shape=(1, 16, 1, 64), dtype=float32)', 'tf.Tensor(shape=(1, 16, 1, 64), dtype=float32)')\n  • layer_head_mask=None\n  • query_length=None\n  • use_cache=True\n  • training=False\n  • output_attentions=False"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(tokenizer\u001b[39m.\u001b[39mbatch_decode(outputs, \n\u001b[0;32m      2\u001b[0m                             skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(outputs, \n",
    "                            skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do this a bit more cleverly by using a single prompt plus a F-string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"classify the following text as positive or negative: {input_text}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we could, for example, write functions for specific tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(input_text:str) -> str:\n",
    "    prompt = f\"classify the following text as positive or negative: {input_text}\"\n",
    "    inputs = tokenizer(prompt, \n",
    "                    max_length = 200,\n",
    "                    return_tensors=\"tf\")\n",
    "    outputs = model.generate(**inputs)\n",
    "    print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(\"I absolutely hated this movie!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "Look through previous notebooks, exercises, and datasets from Language Analytics so far this semester. In small groups, try using either ```Flan-T5``` and **ChatGPT** (or both) try to solve those problems using generative language models.\n",
    "\n",
    "So that would mean, for example:\n",
    "\n",
    "    - Grammatical analysis\n",
    "    - Named entity recognition/extraction\n",
    "    - Classification\n",
    "    - Topic modelling\n",
    "\n",
    "As an illustrative example: try using Flan-T5 to perform classification on the Fake or Real News dataset. How does it perform on ground truth? Is it better or worse than the other classifiers we've seen? How about if we use ChatGPT for the same task?\n",
    "\n",
    "Be creative!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
